Decision Trees
====================================================

We will have a look at the `Carseats` data using the 'tree' package n R, as in the lab in the book.  
We create a binary response variable `High` (for high sales), and we include it in the same dataframe

```{r, warning=FALSE}
require(ISLR)
require(tree)
attach(Carseats)

hist(Sales)
High=ifelse(Sales<=8,"No","Yes")
Carseats=data.frame(Carseats,High)
```

Now we fit a tree to these data, summarize and plot it. Notice that we have to _exclude_ `Sales` from the right hand side of the formula, because the response i derived from it.

```{r}
tree.carseats<-tree(High~.-Sales,data=Carseats)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats,pretty=0)
```

This is a complex tree that is very difficult to interpret.For detailed summary of the tree, print it: Handy to extract details of tree for other purposes.
```{r}
tree.carseats
```

Lets create a training and test set (250,150) split of the 400 observations, grow the tree on the training set and evaluate its performance on the test set.

```{r}
set.seed(1011)
train=sample(1:nrow(Carseats),250)
tree.carseats=tree(High~.-Sales,Carseats,subset=train)
plot(tree.carseats); text(tree.carseats,pretty=0)
#predict class labels
tree.pred<-predict(tree.carseats,Carseats[-train,],type='class')
with(Carseats[-train,],table(tree.pred,High))
#this is the error rate
(72+33)/150
```

This tree was grown to full depth, and might be too variable. We now use CV to prune it.
```{r}
cv.carseats=cv.tree(tree.carseats,FUN=prune.misclass)
cv.carseats
plot(cv.carseats)
prune.carseats<-prune.misclass(tree.carseats,best=13)
plot(prune.carseats); text(prune.carseats,pretty=0)

tree.pred<-predict(prune.carseats,Carseats[-train,],type='class')
with(Carseats[-train,],table(tree.pred,High))
(72+32)/150
```
Only one difference, so pruning did not improve test performance but we got a shallower tree, which is easier to interpret.


Random Forests and Boosting
====================================================

```{r}
require(randomForest)
require(MASS)
set.seed(101)
dim(Boston)
train<-sample(1:nrow(Boston),300)
?Boston
```

Let's fit a random forest and see how well it performs. We will use the response `medv`, the median housing value (in \$1k dollars)

```{r}
rf.boston<-randomForest(medv~.,data=Boston,subset=train)
rf.boston
```

The MSR and % variance explained are based on OOB or _out-of-bag_ estimates, a very clever device in random forests to get honest error estimates. the model reponses that `mtry=4`, which is the number of variables randomly chosen at each split. Since $p=13$ here, we could try all 13 possible values of `mtry`. We will do so, record the results and make a plot.

```{r}
oob.err<-double(13)
test.err<-double(13)
for(mtry in 1:13){
  fit<-randomForest(medv~.,data=Boston, subset=train, mtry=mtry,ntree=400)
  oob.err[mtry]<-fit$mse[400]
  pred<-predict(fit,Boston[-train,])
  test.err[mtry]<-with(Boston[-train,],mean((medv-pred)^2))
  cat(mtry," ")
}

#type b= plot both points and connect them with lines
matplot(1:mtry,cbind(test.err,oob.err),pch=19,col=c('red','blue'),type='b',ylab='Mean Squared Error')
legend('topright',legend=c('OOB',"Test"),pch=19,col=c('red','blue'))
```

Not too difficult! Although the test-error curve drops below the OOB curve, these are estimates based on data, and so have their own standard errors (which are typically quite large). Notice that the points at the end with `mtry=13` correspond to bagging. The curves are quite smooth because the random forest with `mtry=4` is correlated with `mtry=5`.

Boosting
--------

Boosting builds lots of smaller trees. Unlike random forests, each new tree in boosting tries to patch up the deficiencies of the current ensemble.

```{r, warning=FALSE}
require(gbm)
boost.boston<-gbm(medv~.,data=Boston[train,],distribution="gaussian",n.trees=10000,shrinkage=0.01,interaction.depth = 4)
summary(boost.boston)
plot(boost.boston,i='lstat')
plot(boost.boston,i='rm')
```

Let's make a prediction on the test set. With boosting, the number of trees is a tuning parameter, and if we have too many we can overfit. So we should use cross validation to select the number of trees. We will leave this as an exercise. Instead we will compute the test error as a function of the number of trees, and make a plot.

```{r}
n.trees<-seq(from=100,to=10000,by=100)
predmat<-predict(boost.boston,newdata=Boston[-train,],n.trees = n.trees)
dim(predmat)
berr<-with(Boston[-train,],apply( (predmat-medv)^2,2,mean))
plot(n.trees,berr,pch=19,ylab = 'Mean Squared error', xlab='# Trees',main='Boosting Test Error')
abline(h=min(test.err),col='red')
```