---
title: "Homework4"
author: "LinLeeCheong"
date: "March 5, 2016"
output: word_document
---

Problem #1
----------
a)     ------------X1<5-----------  
       |&nbsp;&nbsp;&nbsp;&nbsp; |     
       |&nbsp;&nbsp;&nbsp;&nbsp; |
&nbsp;---X2<1----&nbsp;&nbsp;&nbsp; 5    
   |         |  
--X1<3--&nbsp;15  
|       |   
3   --x2<0---  
    |       |  
    10      0  
    
    
b) 




Problem #2
----------

a) Plot observations
```{r}
x1<-c(3,2,4,1,2,4,4)
x2<-c(4,2,4,4,1,3,1)
y<-c('red','red','red','red','blue','blue','blue')
plot(x1,x2,col=y,pch=19, main='Plotting observations')
grid()
```

b) The hyperplane with maximum margin is one that separates the points with the space. Visually can see it is between the two sets of vertical red/blue points.

The equation, using y=ax+b, would be x2=x1-0.5. Rearranging this gives 0.5-x1+x2=0. However there is the additional rule that for the function f(B0+B1X1+B2*X2), the sum of the beta's B0^2+ B1^2+ B2^2=1. So a scaling factor is needed:S(0.5^2+1+1)=1 --> which S=4/9 --> so all betas need to be scaled by 2/3.

In other words, the maximal margin classifier hyperplane should be 1/3-2x1/3+2x2/3=0
```{r}
plot(x1,x2,col=y,pch=19, main='sketch separating hyperplane')
grid()
abline(-0.5,1,col='green')
```

c) For the blue point (2,1) the equation (2/3)(0.5-x1+x2)=(2/3)(0.5-2+1)=(2/3)(-1/2)=(-1/3)<0 and for the red point (2,2), the equation gives (2/3)(0.5-2+2)=(1/3>0.  

So the classification rule for the maximal margin classifier would be: Classify to Red if 1/3-2x1/3+2x2/3>0, and classify to Blue otherwise.

d) The margin is shown below in dashed green line: Margins are 0.5 units wide in each of the X1 direction and 0.5 units wide in each of the X2 direction.
```{r}
plot(x1,x2,col=y,pch=19, main='sketch margins')
grid()
abline(-0.5,1,col='green')
abline(-1,1,col='green',lty=2)
abline(0,1,col='green',lty=2)
```

e) The support vectors are in diamonds in the plot:
```{r,warning=FALSE}
library(e1071)
dat<-data.frame(X1=x1,X2=x2,y=as.factor(y))
svmfit<-svm(as.factor(y)~.,data=dat,scale=TRUE,cost=100,kernel='linear')
plot(x1,x2,col=y,pch=19, main='support vectors')
grid()
points(dat[svmfit$index,],pch=5,cex=2)
abline(-0.5,1,col='green')
```

f) The seventh observation (4,1) is not a support vector so slight movements will not affect the maximal margin hyperplane.

g) A non optimal hyperplane is shown below:
```{r}
plot(x1,x2,col=y,pch=19, main='non-optimal hyperplane')
grid()
abline(0,0.8,col='green',lty=2)
```

h) Additional observation such that it is not separable by hyperplane:
```{r}
plot(x1,x2,col=y,pch=19, main='not separable ')
grid()
abline(-0.5,1,col='green')
points(3,1.5,col='red',pch=19)
points(3,1.5,pch=5,cex=2)
```


Problem #3
----------

a) Generate simulated data set with 20 observations in 3 classes and 50 variables. Use the same technique showed in the videos for k means
```{r, warning=FALSE}
set.seed(2016)
x=matrix(rnorm(3000),60,50)
xmean=matrix(rnorm(150,sd=0.8),3,50) 
which=sample(1:3,60,replace=TRUE)
x=x+xmean[which,]
```

b) Perform PCA and plot the first two principle component score vectors??
```{r}
pca.p3=prcomp(x,scale=TRUE)
names(pca.p3)
#indeed deviation most in first two directions
pca.p3$sdev
loadings=pca.p3$rotation[,1:2]
newZ=predict(pca.p3)
plot(newZ[,1],newZ[,2],col=which,pch=19)
```

c) Perform k means with K=3. Assuming we know the number of cluster there are (K=3), it is able to separate the clusters well without any misclassifications.
```{r}
set.seed(306)
km.p3=kmeans(x,3,nstart=15)
table(km.p3$cluster,which)
```

d) Perform k means with K=2. Now it puts two clusters in one group, compared to the original 3 clusters.
```{r}
set.seed(306)
km.p3.k2=kmeans(x,2,nstart=15)
table(km.p3.k2$cluster,which)
```

e) Perform k means with K=4. Now it split one of the 3 original clusters into two clusters
```{r}
set.seed(306)
km.p3.k4=kmeans(x,4,nstart=15)
table(km.p3.k4$cluster,which)
```

f) Perform k means with K=3 on the first two principal component score vectors. It actually is still able to cluster them correctly!
```{r}
xwithZ<-newZ[,1:2]
set.seed(306)
km.p3.withZ<-kmeans(xwithZ,3,nstart=15)
table(km.p3.withZ$cluster,which)
```

g) scaling k means. In this instance, they give identical results. This is useful if the components used have very different units and variances, but in this case they came from the same rnorm pool and does not have much of an effect.
```{r}
set.seed(306)
km.p3.scale<-kmeans(scale(x,center = FALSE, scale = apply(x, 2, sd)),3,nstart=15)
table(km.p3.scale$cluster,which)
```


Problem #4
----------
a) Plot test MSE as a function of trees for bagging and random forest
```{r}
library(randomForest)
load("C:/Users/licheong/Documents/Classes_ISLR/body.RData")
test<-sample(1:nrow(X),200)
train<-(1:nrow(X))[-test]
bodyDF<-data.frame(Weight=Y$Weight,X)
bodyRF<-randomForest(Weight~.,data=bodyDF,subset=train,ntree=500,xtest=X[-train,],ytest=Y$Weight[-train])
bodyBag<-randomForest(Weight~.,data=bodyDF,subset=train,ntree=500,xtest=X[-train,],ytest=Y$Weight[-train],mtry=21)
plot(1:500, bodyRF$test$mse,pch=19)
points(1:500,bodyBag$test$mse,pch=19,col='blue')
```

b) Variable importance between randomforest and bagging
```{r}
varImpPlot(bodyRF,cex=0.5)
varImpPlot(bodyBag,cex=0.5)
```

c) Compare test errors:
From previous homework, the `test error(PCR)=9.999427`, `test error (PLS)=9.218095`,`test error(FSS)=8.987679`.  
In comparison, the randomForest was worse than FSS and PLS but worse than PCR.
```{r}
bodyRF$test$mse[500]
```

d) No it would not be useful to include more test trees as the test error flattens out from about 50 trees onwards